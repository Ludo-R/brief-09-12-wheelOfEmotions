{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wheel of emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score, precision_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import FastICA, KernelPCA, TruncatedSVD, SparsePCA, NMF, FactorAnalysis, LatentDirichletAllocation\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from time import time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propose several models of classification of emotions and propose a qualitative and quantitative analysis of these models according to evaluation criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define stopwords and vectorizer\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "vectoriser = CountVectorizer(ngram_range=(1,2), stop_words = stopwords )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define all classification model\n",
    "logreg = LogisticRegression(max_iter = 1000)\n",
    "svclass = SVC()\n",
    "sgdc = SGDClassifier(max_iter = 5000)\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "#define fit and predict function\n",
    "def fitting(X, y, mod):\n",
    "    mod.fit(X, y)\n",
    "\n",
    "def predict(X, mod):\n",
    "    xx = mod.predict(X)\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First have to work with the dataset from Kaggle to carry out your training and the evaluation of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "df = pd.read_csv(\"data/emotion_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21459 entries, 0 to 21458\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Text     21459 non-null  object\n",
      " 1   Emotion  21459 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 335.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define x,y and clean data\n",
    "x = np.array(df[\"Text\"])\n",
    "y = np.array(df[\"Emotion\"])\n",
    "\n",
    "x = vectoriser.fit_transform(x)\n",
    "\n",
    "#define result dict\n",
    "\n",
    "result = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)\n",
    "\n",
    "#fit and predict\n",
    "fitting(x_train, y_train, logreg)\n",
    "ypred = predict(x_test, logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_f1 = f1_score(y_test, ypred, average=\"weighted\")\n",
    "logreg_recall = recall_score(y_test, ypred, average=\"weighted\")\n",
    "logreg_precision = precision_score(y_test, ypred, average=\"weighted\")\n",
    "result['logreg'] = logreg_f1, logreg_recall, logreg_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)\n",
    "\n",
    "fitting(x_train, y_train, svclass)\n",
    "ypred = predict(x_test, svclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "svclass_f1 = f1_score(y_test, ypred, average=\"weighted\")\n",
    "svclass_recall = recall_score(y_test, ypred, average=\"weighted\")\n",
    "svclass_precision = precision_score(y_test, ypred, average=\"weighted\")\n",
    "result['svc'] = svclass_f1, svclass_recall, svclass_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)\n",
    "\n",
    "fitting(x_train, y_train, sgdc)\n",
    "ypred = predict(x_test, sgdc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdc_f1 = f1_score(y_test, ypred, average=\"weighted\")\n",
    "sgdc_recall = recall_score(y_test, ypred, average=\"weighted\")\n",
    "sgdc_precision = precision_score(y_test, ypred, average=\"weighted\")\n",
    "result['sgdc'] = sgdc_f1, sgdc_recall, sgdc_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)\n",
    "\n",
    "fitting(x_train, y_train, knn)\n",
    "ypred = predict(x_test, knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_f1 = f1_score(y_test, ypred, average=\"weighted\")\n",
    "knn_recall = recall_score(y_test, ypred, average=\"weighted\")\n",
    "knn_precision = precision_score(y_test, ypred, average=\"weighted\")\n",
    "result['knn'] = knn_f1, knn_recall, knn_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)\n",
    "\n",
    "fitting(x_train, y_train, dtree)\n",
    "ypred = predict(x_test, dtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_f1 = f1_score(y_test, ypred, average=\"weighted\")\n",
    "dtree_recall = recall_score(y_test, ypred, average=\"weighted\")\n",
    "dtree_precision = precision_score(y_test, ypred, average=\"weighted\")\n",
    "result['dtree'] = dtree_f1, dtree_recall, dtree_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result - DF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1_score - Recall - Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logreg</td>\n",
       "      <td>(0.8991977253696037, 0.9007455731593662, 0.899...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>svc</td>\n",
       "      <td>(0.7825518915306116, 0.7968313140726934, 0.821...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sgdc</td>\n",
       "      <td>(0.9050579787073314, 0.9061043802423113, 0.904...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>knn</td>\n",
       "      <td>(0.45736459471976015, 0.4666821994408201, 0.52...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dtree</td>\n",
       "      <td>(0.8735854595419213, 0.8734855545200373, 0.874...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model                      F1_score - Recall - Precision\n",
       "0  logreg  (0.8991977253696037, 0.9007455731593662, 0.899...\n",
       "1     svc  (0.7825518915306116, 0.7968313140726934, 0.821...\n",
       "2    sgdc  (0.9050579787073314, 0.9061043802423113, 0.904...\n",
       "3     knn  (0.45736459471976015, 0.4666821994408201, 0.52...\n",
       "4   dtree  (0.8735854595419213, 0.8734855545200373, 0.874..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste = []\n",
    "values = []\n",
    "\n",
    "for k, v in result.items():\n",
    "    liste.append(k)\n",
    "    values.append(v)\n",
    "    \n",
    "dfresult = pd.DataFrame(columns=['Model','F1_score - Recall - Precision'])\n",
    "\n",
    "dfresult['Model'] = pd.Series(liste)\n",
    "dfresult['F1_score - Recall - Precision'] = pd.Series(values)\n",
    "\n",
    "dfresult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce pipeline with more preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipes(pipes, splits=10, test_size=0.2, seed=0):  \n",
    "    res = defaultdict(list)\n",
    "    spliter = ShuffleSplit(n_splits=splits, test_size=test_size, random_state=seed)\n",
    "    for idx_train, idx_test in spliter.split(corpus):\n",
    "        for pipe in pipes:\n",
    "            # name of the model\n",
    "            name = \"-\".join([x[0] for x in pipe.steps])\n",
    "            \n",
    "            # extract datasets\n",
    "            X_train = corpus[idx_train]\n",
    "            X_test = corpus[idx_test]\n",
    "            y_train = targets[idx_train]\n",
    "            y_test = targets[idx_test]\n",
    "            \n",
    "            # Learn\n",
    "            start = time()\n",
    "            pipe.fit(X_train, y_train)\n",
    "            fit_time = time() - start\n",
    "            \n",
    "            # predict and save results\n",
    "            y = pipe.predict(X_test)\n",
    "            res[name].append([\n",
    "                fit_time,\n",
    "                f1_score(y_test, y, average=\"weighted\"),\n",
    "                recall_score(y_test, y, average=\"weighted\"),\n",
    "                precision_score(y_test, y, average=\"weighted\")\n",
    "            ])\n",
    "    return res\n",
    "\n",
    "def print_table(res):\n",
    "    # Compute mean and std\n",
    "    final = {}\n",
    "    for model in res:\n",
    "        arr = np.array(res[model])\n",
    "        final[model] = {\n",
    "            \"time\" : arr[:, 0].mean().round(2),\n",
    "            \"f1\": [arr[:,1].mean().round(3), arr[:,1].std().round(3)],\n",
    "            \"recall\": [arr[:,2].mean().round(3), arr[:,1].std().round(3)],\n",
    "            \"precision\": [arr[:,3].mean().round(3), arr[:,1].std().round(3)],}\n",
    "\n",
    "    df = pd.DataFrame.from_dict(final, orient=\"index\").round(3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "df = pd.read_csv(\"data/emotion_final.csv\")\n",
    "\n",
    "corpus = np.array(df['Text'])\n",
    "targets = np.array(df['Emotion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe00 = Pipeline([\n",
    "    ('idf&lda', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "                ('lda', LatentDirichletAllocation(n_components=25)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),\n",
    "                ('t', TfidfTransformer()),            \n",
    "        ]))\n",
    "    ])),\n",
    "    ('logreg', LogisticRegression(max_iter=5000, tol=1e-4)),\n",
    "])\n",
    "pipe001 = Pipeline([\n",
    "    ('idf', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),\n",
    "                ('t', TfidfTransformer()),            \n",
    "        ]))\n",
    "    ])),\n",
    "    ('logreg', LogisticRegression(max_iter=5000, tol=1e-4)),\n",
    "])\n",
    "pipe002 = Pipeline([\n",
    "    ('lda', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "                ('lda', LatentDirichletAllocation(n_components=25)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),      \n",
    "        ]))\n",
    "    ])),\n",
    "    ('logreg', LogisticRegression(max_iter=5000, tol=1e-4)),\n",
    "])\n",
    "pipe003 = Pipeline([\n",
    "    ('only', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),         \n",
    "        ]))\n",
    "    ])),\n",
    "    ('logreg', LogisticRegression(max_iter=5000, tol=1e-4)),\n",
    "])\n",
    "res = run_pipes([pipe00, pipe001, pipe002, pipe003])\n",
    "print_table(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe01 = Pipeline([\n",
    "    ('idf&lda', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "                ('lda', LatentDirichletAllocation(n_components=25)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),\n",
    "                ('t', TfidfTransformer()),            \n",
    "        ]))\n",
    "    ])),\n",
    "    ('svc', SVC(max_iter=5000, tol=1e-4)),\n",
    "])\n",
    "pipe011 = Pipeline([\n",
    "    ('idf', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),\n",
    "                ('t', TfidfTransformer()),            \n",
    "        ]))\n",
    "    ])),\n",
    "    ('svc', SVC(max_iter=5000, tol=1e-4)),\n",
    "])\n",
    "pipe012 = Pipeline([\n",
    "    ('lda', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "                ('lda', LatentDirichletAllocation(n_components=25)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),     \n",
    "        ]))\n",
    "    ])),\n",
    "    ('svc', SVC(max_iter=5000, tol=1e-4)),\n",
    "])\n",
    "pipe013 = Pipeline([\n",
    "    ('only', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),       \n",
    "        ]))\n",
    "    ])),\n",
    "    ('svc', SVC(max_iter=5000, tol=1e-4)),\n",
    "])\n",
    "res = run_pipes([pipe01, pipe011, pipe012, pipe013])\n",
    "print_table(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe02 = Pipeline([\n",
    "    ('idf&lda', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "                ('lda', LatentDirichletAllocation(n_components=25)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),\n",
    "                ('t', TfidfTransformer()),            \n",
    "        ]))\n",
    "    ])),\n",
    "    ('sgd', SGDClassifier(max_iter=5000, tol=1e-4)),\n",
    "])\n",
    "pipe021 = Pipeline([\n",
    "    ('idf', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),\n",
    "                ('t', TfidfTransformer()),            \n",
    "        ]))\n",
    "    ])),\n",
    "    ('sgd', SGDClassifier(max_iter=5000, tol=1e-4)),\n",
    "])\n",
    "pipe022 = Pipeline([\n",
    "    ('lda', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "              ('lda', LatentDirichletAllocation(n_components=25)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),   \n",
    "        ]))\n",
    "    ])),\n",
    "    ('sgd', SGDClassifier(max_iter=5000, tol=1e-4)),\n",
    "])\n",
    "pipe023 = Pipeline([\n",
    "    ('only', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),   \n",
    "        ]))\n",
    "    ])),\n",
    "    ('sgd', SGDClassifier(max_iter=5000, tol=1e-4)),\n",
    "])\n",
    "res = run_pipes([pipe02, pipe021, pipe022, pipe023])\n",
    "print_table(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe03 = Pipeline([\n",
    "    ('idf&lda', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "                ('lda', LatentDirichletAllocation(n_components=25)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),\n",
    "                ('t', TfidfTransformer()),            \n",
    "        ]))\n",
    "    ])),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=10)),\n",
    "])\n",
    "pipe031 = Pipeline([\n",
    "    ('idf', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),\n",
    "                ('t', TfidfTransformer()),            \n",
    "        ]))\n",
    "    ])),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=10)),\n",
    "])\n",
    "pipe032 = Pipeline([\n",
    "    ('lda', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "                ('lda', LatentDirichletAllocation(n_components=25)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),\n",
    "        ]))\n",
    "    ])),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=10)),\n",
    "])\n",
    "pipe033 = Pipeline([\n",
    "    ('only', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),         \n",
    "        ]))\n",
    "    ])),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=10)),\n",
    "])\n",
    "\n",
    "res = run_pipes([pipe03, pipe031, pipe032, pipe033])\n",
    "print_table(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe04 = Pipeline([\n",
    "    ('idf&lda', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "                ('lda', LatentDirichletAllocation(n_components=25)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),\n",
    "                ('t', TfidfTransformer()),            \n",
    "        ]))\n",
    "    ])),\n",
    "    ('dtree', DecisionTreeClassifier(max_iter=5000, tol=1e-4)),\n",
    "])\n",
    "pipe041 = Pipeline([\n",
    "    ('idf', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),\n",
    "                ('t', TfidfTransformer()),            \n",
    "        ]))\n",
    "    ])),\n",
    "    ('dtree', DecisionTreeClassifier(max_iter=5000, tol=1e-4)),\n",
    "])\n",
    "pipe042 = Pipeline([\n",
    "    ('lda', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "                ('lda', LatentDirichletAllocation(n_components=25)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),           \n",
    "        ]))\n",
    "    ])),\n",
    "    ('dtree', DecisionTreeClassifier(max_iter=5000, tol=1e-4)),\n",
    "])\n",
    "pipe043 = Pipeline([\n",
    "    ('only', FeatureUnion([\n",
    "        (\"decomposition\", Pipeline([\n",
    "                (\"c\", CountVectorizer(stop_words=stopwords, min_df=3)),\n",
    "        ])),\n",
    "        (\"tfidf\", Pipeline([\n",
    "                (\"c\", CountVectorizer(ngram_range=(1,2))),            \n",
    "        ]))\n",
    "    ])),\n",
    "    ('dtree', DecisionTreeClassifier(max_iter=5000, tol=1e-4)),\n",
    "])\n",
    "res = run_pipes([pipe04, pipe041, pipe042, pipe043])\n",
    "print_table(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In second have to work with the dataset from Data world to carry out your training and the evaluation of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"data/text_emotion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.array(df2[\"content\"])\n",
    "y2 = np.array(df2[\"sentiment\"])\n",
    "\n",
    "x2 = vectoriser.fit_transform(x2)\n",
    "\n",
    "result2 = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x2, y2, test_size=0.20, random_state=0)\n",
    "\n",
    "fitting(x_train, y_train, logreg)\n",
    "ypred = predict(x_test, logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg2_f1 = f1_score(y_test, ypred, average=\"weighted\")\n",
    "result2['logreg2_f1'] = logreg2_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x2, y2, test_size=0.20, random_state=0)\n",
    "\n",
    "fitting(x_train, y_train, svclass)\n",
    "ypred = predict(x_test, svclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svclass2_f1 = f1_score(y_test, ypred, average=\"weighted\")\n",
    "result2['svclass2_f1'] = svclass2_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x2, y2, test_size=0.20, random_state=0)\n",
    "\n",
    "fitting(x_train, y_train, sgdc)\n",
    "ypred = predict(x_test, sgdc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgdc2_f1 = f1_score(y_test, ypred, average=\"weighted\")\n",
    "result2['sgdc2_f1'] = sgdc2_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x2, y2, test_size=0.20, random_state=0)\n",
    "\n",
    "fitting(x_train, y_train, knn)\n",
    "ypred = predict(x_test, knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2_f1 = f1_score(y_test, ypred, average=\"weighted\")\n",
    "result2['knn2_f1'] = knn2_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x2, y2, test_size=0.20, random_state=0)\n",
    "\n",
    "fitting(x_train, y_train, dtree)\n",
    "ypred = predict(x_test, dtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2_f1 = f1_score(y_test, ypred, average=\"weighted\")\n",
    "result2['knn2_f1'] = knn2_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse - dataframe 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste2 = []\n",
    "values2 = []\n",
    "\n",
    "for k, v in result2.items():\n",
    "    liste2.append(k)\n",
    "    values2.append(v)\n",
    "    \n",
    "dfresult2 = pd.DataFrame(columns=['Model','F1_score'])\n",
    "\n",
    "dfresult2['Model'] = pd.Series(liste2)\n",
    "dfresult2['F1_score'] = pd.Series(values2)\n",
    "\n",
    "print(dfresult2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the one hand, compare whether the classification results on your first dataset are similar with the second. Comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Combine the two datasets to try to improve your prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
